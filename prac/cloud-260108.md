---
tags:
  - topic/cloud
  - type/practice
Date: 2026-01-08
aliases:
  - pv
  - pvc
  - storageclass
  - volume
---
## Volume
![[Pasted image 20260113215234.png]]
[Kubernetes - Volumes](https://kubernetes.io/docs/concepts/storage/volumes/)
- provide a way for containers in a pod to access and share data via the filesystem.
- Why volumes are important
	- Data Persistene
	- Shared Storage
![[Pasted image 20260113215306.png]]
![[Pasted image 20260113215326.png]]
- 사용자가 필요한 스토리지의 크기와 접근 모드(ReadWriteOnce, ReadOnlyMany, ReadWriteMany)를 정의하고, 이를 기반으로 클러스터 내의 적절한 PV에 바인딩
### 1. Volume: `emptyDir`
#### 1-1. Pod 생성
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: jde-pod-volume
  namespace: default
spec:
  volumes:
    - name: secloudit-log-volume
      emptyDir: {}                 # Pod가 생성될 때 생성되는 임시 볼륨 정의
  containers:
    - name: secloudit-log-writer
      image: busybox
      command: ["/bin/sh", "-c"]
      args:
        - while true; do
            echo "$(date)" >> /var/log/secloudit.log;
            sleep 5;
          done
      volumeMounts:
        - name: secloudit-log-volume
          mountPath: /var/log
    - name: secloudit-log-reader
      image: busybox
      command: ["/bin/sh", "-c"]
      args:
        - tail -f /var/log/secloudit.log
      volumeMounts:
        - name: secloudit-log-volume
          mountPath: /var/log
```
```sh
# PV를 정의한 Pod 생성
$ kubectl apply -f volume.yaml

# Pod 목록 조회
$ kubectl get pods
```
![[Pasted image 20260108104443.png]]
- 로그를 Pod 내의 컨테이너에 Volume을 mount하여 쌓음
#### 1-2. Pod 접속하여 로그 조회
```sh
# Pod 접속하여 shell 프로세스 실행
$ kubectl exec -it jde-pod-volume -- /bin/sh
```
![[Pasted image 20260108104620.png]]
- 설정한 `mountPath: /var/log`에 로그가 쌓이는 것을 확인할 수 있다.
### 2. Volume: `hostPath`
#### 2-1. Pod 생성
```yaml
# volume2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: jde-pod-volume2
  namespace: default
spec:
  containers:
  - name: secloudit-log-writer    
    image: busybox
    command: ["sh", "-c"]
    args:
    - while true; do
      echo "$(date)" >> /var/log/secloudit.log;
      sleep 10;
      done
    volumeMounts:      # 컨테이너 내부에 volume을 마운트하는 설정
    - name: secloudit-log-volume   # volume 지정 (name 동일해야 함)
      mountPath: /var/log          # 컨테이너 내부에서 volume을 사용할 위치
  volumes:
  - name: secloudit-log-volume
    hostPath:
      path: /var/log/secloudit-logs   # Pod가 스케줄된 Worker Node의 local fs 경로
      type: DirectoryOrCreate         # 디렉토리가 없으면 자동 생성
```
- `hostPath`: 노드에 강하게 결합되어 Pod 재스케줄 시 로그 사라질 수 있음
```sh
# Pod 생성
$ kubectl apply -f volume2.yaml

# Pod 조회 (wide 옵션으로 조회하여 스케줄된 클러스터 확인)
$ kubectl get pods -o wide 
```
#### 2-2. 로그 조회: Worker Node 접속 or Pod 접속
![[Pasted image 20260108105019.png]]
- `jde-pod-volume2`는 `jde-cluster4`에 스케줄되었으므로 
- `jde-cluster4` 접속
![[Pasted image 20260108105304.png]]
- `ubuntu@jde-cluster4` 이름으로 접속하기 위해 `/etc/hosts`에  Private IP와 hostname 추가
![[Pasted image 20260113224210.png]]
- `jde-cluster4:/var/log/secloudit-logs`와 
- `jde-pod-volume2`에 접속하여 `/var/log`로 확인한 로그 결과는 동일하다.
- `hostPath` 필드를 지정하여, 컨테이너 내부(`/var/log`)에 생성된 로그가 Worker Node의 `/var/log/secloudit-logs` 디렉토리에 동일하게 저장되었음을 확인할 수 있다.
![[Pasted image 20260108105428.png]]
#### 2-3. Yaml: Pod가 스케줄링될 Worker Node(클러스터) 지정
```yaml
# volume2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: jde-pod-volume2
  namespace: default
spec:
  containers:
  - name: secloudit-log-writer
    image: busybox
    command: ["sh", "-c"]
    args:
    - while true; do
      echo "$(date)" >> /var/log/secloudit.log;
      sleep 10;
      done
    volumeMounts:
    - name: secloudit-log-volume
      mountPath: /var/log
  volumes:
  - name: secloudit-log-volume
    hostPath:
      path: /var/log/secloudit-logs
      type: DirectoryOrCreate
  nodeSelector:
    kubernetes.io/hostname: jde-cluster2   # hostname(클러스터) 지정
```
```sh
# Pod 조회 (지정한 클러스터에 생성)
$ kubectl get pods -o wide
```
![[Pasted image 20260108111001.png]]
#### 2-4. 지정한 클러스터에 접속하여 로그 조회
```sh
# Pod가 스케줄된 Worker Node에 접속
$ ssh ubuntu@jde-cluster2 -i ~/.ssh/id_rsa
```
![[Pasted image 20260108111249.png]]
### 3. PersistentVolume: Static Provisioning
- `emptyDir` 및 `hostPath`와 같이 Pod 또는 Node에 종속된 볼륨을 사용하는 경우에는 별도의 storage 관리 객체가 필요하지 않았음
- 하지만 Persistent Volume을 구성하기 위해서는 PV, PVC, Storage를 사용해야 함.
- 특히 `no-provisioner`를 사용하여 정적 PV를 구성하는 경우 
	- PV + PVC + StorageClass 모두 필요
#### 3-1. PV + PVC + StorageClass
```sh
# volume4.yaml

# StorageClass (Static Provisioning 명시)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: jde-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
# PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: jde-pv
spec:
  storageClassName: jde-storageclass
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  nfs:
    path: "/home/share/nfs"
    server: 192.168.1.9
---
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jde-pvc
spec:
  storageClassName: jde-storageclass
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
```
```sh
# StorageClass, PV, PVC 생성
$ kubectl apply -f volume4.yaml

# 조회
$ kubectl get storageclass,pv,pvc
```
![[Pasted image 20260108112616.png]]
#### 3-2. Pod 생성
```sh
apiVersion: v1
kind: Pod
metadata:
  name: jde-pod-volume4
  labels:
    name: app
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "Platform as a service!" > /mnt/secloudit-nfs.log && sleep 3600']
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /mnt
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:       # Pod에 미리 생성된 PV를 마운트할 때 사용하는 필드
        claimName: jde-pvc         # pvc 이름 지정

```
- `persistentVolumeClaim: claimName:`
	- Pod에 영구적으로 데이터를 저장할 수 있는 스토리지 연결
#### 3-3. NFS 접속하여 volume 조회
![[Pasted image 20260108113026.png]]
![[Pasted image 20260108113219.png]]
- `cluster5`는 NFS(스토리지)이므로 `.ssh/ssu-key.pem`으로 접속해야 함.
#### 3-4. Pod 접속하여 volume 조회
- `yaml`파일에서 설정한 `mountPath: /mnt`에 마운트되었음을 확인인
![[Pasted image 20260108113339.png]]
### 4. PersistentVolume: Static Provisioning (2)
#### 4-1. 모든 Worker Node에 volume 디렉토리 생성
- 각 Worker Node에 PV를 미리 만들어 두고 수동으로 연결결
```sh
# Worker Node 1 (jde-cluster2)
$ ssh ubuntu@jde-cluster2 ~/.ssh/id_rsa
$ sudo mkdir -p /data/volumes/pv1
$ sudo chmod 777 /data/volumes/pv1

# 나머지 Worker Node에도 동일하게

# Worker Node 2 (jde-cluster3)
$ ssh ubuntu@jde-cluster3 ~/.ssh/id_rsa
$ sudo mkdir -p /data/volumes/pv2
$ sudo chmod 777 /data/volumes/pv2

# Worker Node 3 (jde-cluster4)
$ ssh ubuntu@jde-cluster4 ~/.ssh/id_rsa
$ sudo mkdir -p /data/volumes/pv3
$ sudo chmod 777 /data/volumes/pv3
```
![[Pasted image 20260108115709.png]]
![[Pasted image 20260108115821.png]]
![[Pasted image 20260108115904.png]]
- 실습에서는 `chmod 777`로 권한을 모두 열어두었지만
- 실제로는 `securityContext.fsGroup` 필드로 볼륨 접근에 사용할 GID를 지정한다.
> [Kubernetes Documentation - Configure volume permission and ownership change policy for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods)
> By default, Kubernetes recursively changes ownership and permissions for the contents of each volume to match the `fsGroup` specified in a Pod's `securityContext` when that volume is mounted.

> [Kubernetes Documentation - DIscussion `fsGroup`](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#discussion)
> `fsGroup`: Volumes that support ownership management are modified **to be owned and writable by the GID** specified in `fsGroup`. See the [Ownership Management design document](https://git.k8s.io/design-proposals-archive/storage/volume-ownership-management.md) for more details.
#### 4-2. Storage Class + PV + PVC
- 공통 StorageClass 정의
- 각 Worker Node의 로컬 `pv` 디렉토리를 매핑하기 위한 PV/PVC 생성
```yaml
# volume3.yaml

# 공통 StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: secloudit-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true' # default storage class
provisioner: kubernetes.io/no-provisioner     # static provisioning
reclaimPolicy: Delete  # PVC 삭제 시 데이터와 PV 리소스도 함께 삭제
volumeBindingMode: Immediate    # PVC가 생성되자마자 알맞은 PV를 찾아 즉시 binding
---
# PersistentVolume 1 (jde-cluster2)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: secloudit-pv1
spec:
  storageClassName: secloudit-storageclass   # StorageClass
  persistentVolumeReclaimPolicy: Delete
  capacity:
    storage: 1G
  accessModes:
    - ReadWriteOnce    # 하나의 Node에서만 RW 가능
  local:
    path: /data/volumes/pv1  # local: 네트워크 스토리지가 아닌 로컬 파일 시스템 경로
  nodeAffinity:   # nodeAffinity: 해당 노드에만 존재함을 명시
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname    # hostname 명시시
          operator: In
          values:
          - jde-cluster2
---
# PersistentVolume 2 (jde-cluster3)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: secloudit-pv2
spec:
  storageClassName: secloudit-storageclass
  persistentVolumeReclaimPolicy: Delete
  capacity:
    storage: 2G
  accessModes:
    - ReadWriteOnce
  local:
    path: /data/volumes/pv2
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - jde-cluster3
---
# PersistentVolume 3 (jde-cluster4)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: secloudit-pv3
spec:
  storageClassName: secloudit-storageclass
  persistentVolumeReclaimPolicy: Delete
  capacity:
    storage: 3G
  accessModes:
    - ReadWriteOnce
  local:
    path: /data/volumes/pv3
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - jde-cluster4
---
# PersistentVolumeClaim(PVC) 1 
# strageClass: secloudit-storageclass && metadata.name: secoudit-pv1인 PV 바인딩
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: secloudit-pvc1
spec:
  storageClassName: secloudit-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1G
---
# PersistentVolumeClaim(PVC) 2
# strageClass: secloudit-storageclass && metadata.name: secoudit-pv2인 PV 바인딩
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: secloudit-pvc2
spec:
  storageClassName: secloudit-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2G
---
# PersistentVolumeClaim(PVC) 3
# strageClass: secloudit-storageclass && metadata.name: secoudit-pv3인 PV 바인딩
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: secloudit-pvc3
spec:
  storageClassName: secloudit-storageclass
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3G
```
```sh
# StorageClass, PVC, PV 생성
$ kubectl apply -f volume3.yaml

# pv, pvc 조회
$ kubectl get pv,pvc
# pv, pvc의 STATUS가 Bound임을 확인
```
![[Pasted image 20260108120952.png]]
![[Pasted image 20260108121007.png]]
#### 4-3. Pod 생성 및 Volume 마운트 (Local Volume & NFS)
```yaml
# pod-volume3.yaml

# Pod 1 
apiVersion: v1
kind: Pod
metadata:
  name: secloudit-pod1
  labels:
    name: app
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "Platform as a service!" > /mnt/secloudit-1.log && sleep 3600']
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /mnt    # 컨테이너 내부의 /mnt 경로에 volume 마운트
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:
        claimName: secloudit-pvc1 # pvc1 (jde-cluster2의 pv) 사용
---
# Pod 2
apiVersion: v1
kind: Pod
metadata:
  name: secloudit-pod2
  labels:
    name: app
spec:
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /mnt/secloudit-2.log;
          sleep 10;
        done
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /mnt
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:
        claimName: secloudit-pvc2  # pvc2 (jde-cluster3의 pv) 사용
---
# Pod 3
apiVersion: v1
kind: Pod
metadata:
  name: secloudit-pod3
  labels:
    name: app
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /var/log/nginx    # nginx의 기본 마운트 경로
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:         # nginx의 mountPath를 pv로 마운트 > 영구저장
        claimName: secloudit-pvc3    # pvc3 (jde-cluster4의 pv) 사용
---
# PV, PVC 정적 프로비저닝(NFS)

# NFS용 StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: secloudit-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
# NFS PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: secloudit-pv
spec:
  storageClassName: secloudit-storageclass
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany  # 네트워크 스토리지이므로 여러 노드에서 동시에 읽고 쓰기 가능
  nfs:
    path: "/home/share/nfs" # NFS 서버 내부의 실제 공유 경로
    server: 192.168.9.109
---
# NFS PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: secloudit-pvc
spec:
  storageClassName: secloudit-storageclass
  accessModes:
    - ReadWriteMany  # PV와 마찬가지로 RWX 모드를 요청해야 바인딩됨
  resources:
    requests:
      storage: 1Gi
---
# NFS 테스트 Pod
apiVersion: v1
kind: Pod
metadata:
  name: secloudit-pod
  labels:
    name: app
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "Platform as a service!" > /mnt/secloudit-nfs.log && sleep 3600']
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /mnt
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:
        claimName: secloudit-pvc
```

| Local Volume                    | NFS                          |
| ------------------------------- | ---------------------------- |
| 특정 Node (`nodeAffinity`)만 접근 가능 | 네트워크를 통해 공유되므로 여러 노드에서 접근 가능 |
| AccessMode: `ReadWriteOnce`     | AccessMode: `ReadWriteMany`  |
```sh
$ kubectl apply -f pod-volume3.yaml
$ kubectl get pod -o wide
```
![[Pasted image 20260108121217.png]]
![[Pasted image 20260108121316.png]]
```sh
# Worker Node 1 (jde-cluster2)
$ ssh ubuntu@jde-cluster2 ~/.ssh/id_rsa
$ cat /data/volumes/pv1/secloudit-1.log

# Master Node (jde-cluster1)
$ kubectl exec -it secloudit-pod1 -- sh
$ ls /mnt
# 설정한 mountPath 하위에 secloudit-1.log 파일 확인 가능
```
![[Pasted image 20260108121421.png]]
```sh
# Worker Node 2 (jde-cluster3)
$ ssh ubuntu@jde-cluster3 ~/.ssh/id_rsa
$ cat /data/volumes/pv2/secloudit-2.log

# Master Node (jde-cluster1)
$ kubectl exec -it secloudit-pod2 -- sh
$ ls /mnt
# 설정한 mountPath: /mnt 하위 secloudit-2.log 파일 확인 가능능
```
![[Pasted image 20260108121513.png]]
```sh
# Worker Node 3 (jde-cluster4)
$ ssh ubuntu@jde-cluster4 ~/.ssh/id_rsa
$ cat /data/volumes/pv3/access.log
$ cat /data/volumes/pv3/error.log
# Pod를 삭제하더라도 해당 경로에 로그 파일이 영구적으로 남아있음

# Master Node (jde-cluster1)
$ kubectl exec -it secloudit-pod3 -- sh
$ ls /var/log/nginx
```
![[Pasted image 20260108121610.png]]
##### NFS troubleshooting
- https://github.com/K-PaaS/container-platform/blob/master/install-guide/nfs-server-install-guide.md
- `~/values.yaml`
![[Pasted image 20260108123436.png]]
- NFS Server IP 오타 $\rightarrow$ `192.168.1.9`로 수정
![[Pasted image 20260108123854.png]]
![[Pasted image 20260108123906.png]]
- `helm` 재설치
![[Pasted image 20260108123948.png]]
![[Pasted image 20260108124013.png]]
- `jde-ssu-storageclass` 생성
### 5. PersistentVolume: Dynamic Provisioning
- Dynamic Provisioning의 경우 PV를 구성할 필요 없음 (PVC + StorageClass)
	- PVC 배포하면 StorageClass가 알아서 PV 만듦
```yaml
# volume5.yaml

# PVC (Dynamic)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: secloudit-pvc-dynamic
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: jde-ssu-storageclass 
  # storageClass에 지정된 provisioner가 자동으로 PV 생성 + Binding
  resources:
    requests:
      storage: 1Gi
# PV 생성하지 않음
---
# Pod
apiVersion: v1
kind: Pod
metadata:
  name: secloudit-pod-dynamic
  labels:
    name: app
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "Platform as a service!" > /mnt/secloudit.log && sleep 3600']
    volumeMounts:
      - name: secloudit-local-volume
        mountPath: /mnt
  volumes:
    - name: secloudit-local-volume
      persistentVolumeClaim:
        claimName: secloudit-pvc-dynamic  # 동적 PVC 연결
```
```sh
$ kubectl apply -f volume5.yaml
$ kubectl get pods -o wide

$ ssh ubuntu@jde-cluster5 ~/.ssh/ssu-key.pem
```
![[Pasted image 20260108125241.png]]
![[Pasted image 20260108125303.png]]
![[Pasted image 20260108131028.png]]
- `jde-cluster5` `/home/share/nfs` 공간에 `secloudit-pvc-dynamic-pvc` 자동 생성
