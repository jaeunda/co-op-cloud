---
Date: 2026-01-07
tags:
  - type/summary
  - topic/cloud
  - type/practice
aliases:
  - service
  - deployment
---
## 03. Network
```
Deployment  → Pod 생성/확장
Service     → Pod 묶음 + 내부 로드밸런싱
Ingress     → 외부 HTTP 진입 + 라우팅
```
### Service
[Kubernetes Documentation - Services, Load Balancing, and Networking](https://kubernetes.io/docs/concepts/services-networking/)
> *The Service API lets you provide **a stable (long lived)** IP address or hostname*
> *for a service implemented by one or more backend pods,*
> *where the individual pods making up the service can change over time*

- Kubernetes Service 객체
	- 네트워크 레벨의 추상화 객체
	- 같은 역할을 하는 Pod 집합을 하나의 네트워크 대상으로 추상화
```
Deployment
  └ ReplicaSet
      └ Pod x N  (같은 기능)
Service
  └ 위 Pod들을 라벨로 묶음 (Forwarding을 위해)
```
- 클러스터 내부에서 고정된 IP 또는 DNS(hostname)을 가짐
	- 실제 처리는 하나 이상의 backend Pod가 담당
- Pod가 죽거나 새로 생겨도, Service IP & DNS 유지.
	- 각 Pod는 고유한 Pod IP를 가지지만 재시작 시 변경되므로 Service가 필요
	- Pod의 구성은 시간에 따라 바뀔 수 있음

```
Client → Service(IP/DNS) → Pod A / Pod B / Pod C
```
- Service는 **label selector**를 이용해 여러 Pod를 묶는다.
- 트래픽을 자동으로 분산 (로드밸런싱)
	- `kube-proxy`:  트래픽을 어떤 Pod로 보낼지 결정
<img width="2241" height="810" alt="Pasted image 20260107191733" src="https://github.com/user-attachments/assets/f399d5b9-df68-47f1-b3e9-39d637dd316a" />

- 라벨을 가진 Pod를 묶어 단일 엔드포인트 제공
	- Service type
		- `ClusterIP`: 클러스터 내부 전용 (cluster-internal IP)
		- `NodePort`: 모든 노드의 특정 포트로 외부 접근 (each Node's IP at a static port)
		- `LoadBalancer`: external load balancer와 연동
		- `ExternalName`: 외부 DNS 이름을 Service처럼 사용
```
	ClusterIP: None   # Headless Service
	
	# Service IP가 직접 할당되지 않고
	# DNS 조회 시 연결된 Pod들의 실제 IP 목록을 직접 반환
	# 로드 밸런싱이 필요 없고, 클라이언트가 특정 Pod와 직접 통신해야 하는 경우 사용
		# DB의 Master - Worker 복제
		# StatefulSet
```
- Pod 간의 로드밸런싱/멀티 포트 지원
	- `로드밸런싱`: **kube-proxy**(iptable/ipvs)가 처리하며 기본적으로 round-robin 방식
	- `멀티 포트`: 하나의 Service에서 여러 포트 노출 가능


<img width="1917" height="965" alt="Pasted image 20260107204958" src="https://github.com/user-attachments/assets/ea19b141-828b-45b4-8ce5-ffd1f4df4649" />

1. 외부 접속 (서버 외부 $\rightarrow$ `nodePort`)
	- 사용자가 웹 브라우저나 외부 클라이언트를 통해 서버에 요청을 보냄
	- 요청은 쿠버네티스 클러스터의 특정 노드(서버) IP와 `nodePort` 포트로 들어옴
2. 서비스 계층 진입 (`nodePort` $\rightarrow$ `port`)
	- 31100번 포트로 들어온 트래픽은 쿠버네티스 Service 객체로 전달
	- 서비스 내부에서는 이 트래픽을 자신의 `port` (80번)로 받음.
3. 파드로 전달 (`port` $\rightarrow$ `targetPort`)
	- Service는 들어온 요청을 실제 작업을 수행할 Pod로 연결
	- 설정된 규칙에 따라 트래픽을 Pod의 `targetPort`로 전달 (Forwarding)
4. 컨테이너 도달 (`targetPort` $\rightarrow$ `Container Port`)
	- Pod 내부로 들어온 요청은 실제 구동 중인 애플리케이션(컨테이너에 도달)
	- Pod 내에서 Nginx 컨테이너가 `containerPort` (80번)를 열고 대기, 최종적으로 Nginx가 요청을 받아 처리
```
외부 사용자
 → NodeIP:31100 (nodePort)
 → Service:80 (port)
 → Pod:80 (targetPort)
 → Nginx Container
```
### Ingress
- 외부 HTTP 요청을 받아 적절한 Service로 라우팅
##### Service vs. Ingress

|     | Service      | Ingress         |
| --- | ------------ | --------------- |
| 계층  | L4 (TCP/UDP) | L7 (HTTP/HTTPS) |
| 대상  | Pod          | Service         |
| 기능  | 로드밸런싱        | 라우팅             |
| 기준  | IP + Port    | Host / Path     |
| TLS | X            | O               |
```
[외부 사용자]  →  Ingress  →  Service  →  Pods들
```
---
### Service - 실습
#### 1. L7 Load Balancing: Ingress - Service(`ClusterIP`) - Deployment
##### 1-1. Deployment 생성 (p.206)
- Kubernetes Deployment 리소스 정의 및 생성
- Service와 연결하기 위해 Backend 역할을 할 Pod를 띄움
```sh
# deploy-svc.yaml

apiVersion: apps/v1
kind: Deployment    # 리소스의 종류: Deployment(Self-healing, scaling)
metadata:
  name: jde-deploy-svc
  namespace: jde-ns # default 공간이 아니라 별도 namespace에 격리되어 생성
  labels:
    app: hello      # 리소스를 식별하는 키-값 쌍
spec:
  replicas: 3       # 동일한 Pod 3개 유지
  selector:
    matchLabels:
      app: hello    # 연결할 Pod의 레이블 키-값
  template:
    metadata:
      labels:
        app: hello  # Pod의 레이블 키-값
    spec:
      containers:
      - name: nginx
        image: nginxdemos/hello:plain-text  # 컨테이너 이미지
        ports:
        - name: http
          containerPort: 80  # 컨테이너 내부에서 80번 포트를 연다
          protocol: TCP
```
- `selector`
	- `matchLabels: app: hello`: 이 Deployment는 label이 `app: hello`인 Pod만 관리한다.
- `template` - 실제로 생성될 Pod의 설계도
	- `metadata: labels: app: hello` 해당 label을 붙여서 생성하므로 
	- 위 `selector`가 이 Pod들을 관리 대상으로 인식
```sh
# Deployment 생성
$ kubectl create -f deploy-svc.yaml
$ kubectl apply -f deploy-svc.yaml

# jde-ns에 해당 Pod(3개)가 생성되었는지 확인
$ kubectl get pods -n jde-ns

# Deployment 목록 조회
$ kubectl get deployment
$ kubectl describe deploy jde-deploy-svc
```
<img width="2423" height="515" alt="Pasted image 20260107104304" src="https://github.com/user-attachments/assets/bc50aabb-a1f0-4aca-abdb-0d46ccc736ea" />

- `jde-deploy-svc-*` Pod 3개 생성
- 해당 Pod로 트래픽을 분산시키기 위해
	- Service 생성 $\rightarrow$ Label Selector로 `app: hello` 지정 (해당 Pod와 동일하게)
	- $\therefore$ Service로 들어오는 트래픽이 3개의 Pod로 로드밸런싱
##### 1-2. Service 생성 (p. 211)
- Service 리소스 정의 및 생성
- 앞서 생성한 Deployment의 Pod와 Service 연결 (`spec: selector:`)
```sh
# svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: jde-svc
  namespace: jde-ns     # Pod와 같은 namespace
  labels:
    app: hello
spec:                   # Service의 동작 방식 정의
  type: ClusterIP       # Service 타입 - 내부 전용
  ports:
  - name: http
    protocol: TCP       # 프로토콜
    port: 8080          # Service 포트 (노출 포트)
    targetPort: 80      # Pod 포트
  selector:
    app: hello          # Deployment 설정과 동일하게
    # jde-ns 안에서 `app=hello` label을 가진 Pod들의 IP 목록(Endpoint)를 실시간 추적
```
- `type: ClusterIP`
	- 외부 연결용 IP를 할당하지 않고, 클러스터 내부에서만 통신 가능한 가상 IP 발급
	- 외부 접속을 위해서는 `NodePort`나 `LoadBalancer` 타입
- `port` vs. `targetPort`
	- `port: 8080` - Service 포트
		- 다른 Pod들이 Service에 접근할 때 사용하는 포트
	- `targetPort: 80` - Pod 포트
		- Service가 트래픽을 받아서 실제 Pod(Nginx)에게 넘겨줄 때 사용하는 포트
	- 들어오는 트래픽(`8080`) $\rightarrow$ Service $\rightarrow$ 나가는 트래픽(`80`)
```sh
# Service 생성
$ kubectl create -f svc.yaml
$ kubectl apply -f svc.yaml

# Service 목록 조회
$ kubectl get service -n jde-ns
```
<img width="1550" height="699" alt="Pasted image 20260107104559" src="https://github.com/user-attachments/assets/6a24a5eb-3669-4a10-ba45-071373117cf7" />

- `jde-svc`의 cluster IP (내부 IP): `10.233.24.133`
- `ServiceIP:ServicePort`로 curl 요청 (`10.233.24.133:8080`)
	- 첫 번째 Res: `10.233.76.129:80` (`jde-deploy-svc-*`)
	- 두 번째 Res: `10.233.127.115:80`(`jde-deploy-svc-*`)
- Service가 Pod 3개 중 서로 다른 Pod로 트래픽 분산
	- 세 개의 Pod를 번갈아 가면서 접근한다. (Round Robin)
	- 참고: [kube-proxy](../notes/service-kube-proxy)
<img width="1933" height="426" alt="Pasted image 20260107104643" src="https://github.com/user-attachments/assets/60073b52-602f-41c2-8106-ea70c69eeef0" />

```sh
# curl 요청 시 Server address로 출력된 IP는
# 해당 요청을 처리한 Pod의 고유한 내부 IP(Endpoint)
# 실제 컨테이너가 할당받은 IP

# Pod의 IP 확인
$ kubectl get pods -n jde-ns -o wide
```
##### 1-3. 내부 통신 테스트
- `Cluster IP`는 클러스터 내부에서만 접근 가능하므로 클러스터 내부의 다른 Pod에서 접근 테스트
- `kubectl run` 명령을 통해 임시로 Pod 생성(Client)
	- 임시 Pod 내에서 `jde-svc`로 `curl` 요청
```sh
# test Pod 생성
$ kubectl run -it test --image=posquit0/doraemon bash -n jde-ns
```
- `--image=posquit0/doraemon`: `curl`, `nslookup` 등 네트워크 도구가 설치된 유틸리티 이미지
- `bash`: Pod가 뜨자마자 Shell을 실행해 접속
- `-n jde-ns`: 같은 namespace 안에 만들어야 DNS 이름으로 Pod를 찾을 수 있음
<img width="2427" height="228" alt="Pasted image 20260107105048" src="https://github.com/user-attachments/assets/3838c4e3-a24c-4758-9231-4cd641479e39" />

```sh
# test Pod 내에서 Service 호출
$ curl jde-svc:8080
```
- 같은 namespace 내에서는 Service IP를 몰라도 Service DNS로 호출 가능
	- Kubernetes CoreDNS가 `jde-svc`를 Service IP로 해석해줌
- [Kubernetes Github - Networking Pod to Pod](https://github.com/kubernetes/design-proposals-archive/blob/main/network/networking.md#pod-to-pod)
	- namespace가 달라도 Pod끼리 통신할 수 있음 (Service Discovery, DNS 해석 방식 차이)
	- we create a NAT-less, flat address space
		- by making IP addresses and ports the same both inside and outside the pods
<img width="1794" height="801" alt="Pasted image 20260107105127" src="https://github.com/user-attachments/assets/061705d6-4460-4e1e-a475-91a8c93b8f4f" />

- 3개의 Pod가 번갈아가며 응답 (Round Robin)
	- `Server address: 10.233.76.129:80`
	- `Server address: 10.233.127.115:80`
	- `Server address: 10.223.112.68:80`
- $\therefore$ Cluster Network 내에서는 DNS 이름만으로 통신이 가능하며, 로드밸런싱도 정상 작동
<img width="1993" height="171" alt="Pasted image 20260107110159" src="https://github.com/user-attachments/assets/6372f133-8697-4d9e-9e42-db87b3c643d8" />

- Service는 ClusterIP만 존재하여 클러스터 내부에서만 접속 가능, 외부 노출 차단 (Backend)
	- `ClusterIP` 타입은 클러스터 내부의 SDN(Software Defined Network) 대역의 IP만 할당
	- Kubernetes 노드 밖에서는 라우팅 테이블에 존재하지 않는 IP이므로 외부에서 직접 호출 불가
- 외부 접속을 위해서는 Ingress (Gateway)가 필요
	- `LoadBalancer` 타입
	- `ingress-nginx-controller`는 `EXTERNAL-IP` 존재 (Single Entry Point)
	- `FLOATING-IP`를 가짐 (Public Entry)
```
[ L7 Routing ]
외부 요청 → Ingress → 내부 연결(Proxy Pass) → Service로 트래픽을 라우팅 
```
- 외부 사용자는 (Ingress Controller를 통해) Ingress로 접속하고, 
- Ingress가 내부의 `jde-svc`로 트래픽을 라우팅(L7)하는 구조

|       | Floating IP | External IP            | Cluster IP            |
| ----- | ----------- | ---------------------- | --------------------- |
|       | Public IP   | Private IP             | Virtual IP            |
| 통신 범위 | 인터넷 전체      | NHN 클라우드 내부망(VPC)      | Kubernetes Cluster 내부 |
| 접속 주체 | 외부 사용자      | Cloud Gateway (Router) | 내부 Pod, Ingress       |

#### 2. L4 Load Balancing: Service(`LoadBalancer`) - Deployment

|        | Ingress - `ClusterIP` Service                  | `LoadBalancer` Service                                         |
| ------ | ---------------------------------------------- | -------------------------------------------------------------- |
| IP     | Ingress의 Public IP 하나로 여러 Service 처리           | Service마다 Public IP 하나씩                                        |
| 트래픽 흐름 | `Floating IP` <br>→ `Ingress`<br>→ `ClusterIP` | `Floating IP` <br>→ `LoadBalancer` <br>→ `Service` <br>→ `Pod` |
##### 2-1. Deployment 정의 및 생성 (p.220)
- 실제 작업을 처리할 Nginx Pod 생성
```sh
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jde-deploy-svc2
  namespace: jde-ns
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```
<img width="2451" height="667" alt="Pasted image 20260107110439" src="https://github.com/user-attachments/assets/afedb531-860a-45f3-bfd4-241f5712b7f3" />

```sh
# Deployment 배포
$ kubectl apply -f deploy-svc2.yaml
$ kubectl create -f deploy-svc2.yaml

# Pod 생성 확인
$ kubectl get pod -n jde-ns
```
##### 2-2. `LoadBalancer` Service 생성 (p.225)
```sh
apiVersion: v1
kind: Service
metadata:
  name: jde-svc2
  namespace: jde-ns
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
```
- `type: LoadBalancer`
	-  외부 사용자는 `Service` 자체가 가진 Floating IP(Public IP)로 직접 들어옴.
	- Service가 바로 Pod로 트래픽 꽂아줌.
```sh
# Service 생성
$ kubectl create -f svc2.yaml
$ kubectl apply -f svc2.yaml
```
<img width="1812" height="326" alt="Pasted image 20260107110645" src="https://github.com/user-attachments/assets/45833263-755b-45d5-987a-0b6238731e4a" />

- `jde-svc2`가 `LoadBalancer` 타입으로 생성
- `EXTERNAL-IP`가 존재하나 물리적으로 연결되지 않은 상태
##### 2-3. 네트워크 인터페이스 생성 및 물리적 연결
- NHN 콘솔에서 `cluster1` 중지
- 네트워크 인터페이스 생성하고 다시 시작
<img width="1064" height="324" alt="Pasted image 20260107110812" src="https://github.com/user-attachments/assets/d3783a91-c540-4955-85e2-e4122fab5db4" />
<img width="1600" height="336" alt="Pasted image 20260107114552" src="https://github.com/user-attachments/assets/e0af7b8d-6fca-4486-80d0-0ff4c449dec7" />

- (참고) `LoadBalancer` 타입의 Service가 생성될 때 지정된 IP 대역에서 `EXTERNAL-IP` 자동 할당
	- 지정 IP는 1씩 incr
	- <img width="1753" height="1112" alt="Pasted image 20260107114730" src="https://github.com/user-attachments/assets/13bb08fa-7ad7-4e15-9410-9fea93948eca" />

###### `LoadBalancer` IP 고정하기  (Static IP Allocation)
- `EXTERNAL-IP`를 임의로 할당하기 위해서는 `svc2.yaml`에 `loadBalancerIP:` 필드 추가
	- <img width="1490" height="635" alt="Pasted image 20260107115409" src="https://github.com/user-attachments/assets/79077cba-1b39-48fd-9496-c12a07db09e8" />

```sh
# LoadBalancer Service의 EXTERNAL-IP를 정적으로 설정하기 위해 
# svc2.yaml 변경

apiVersion: v1
kind: Service
metadata:
  name: jde-svc2
  namespace: jde-ns
spec:
  type: LoadBalancer
  loadBalancerIP: 192.168.1.155   # IP 지정
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
```
<img width="1561" height="309" alt="Pasted image 20260107115659" src="https://github.com/user-attachments/assets/c67fdfca-31a1-47de-9aab-61a8fc80fcb4" />

#### 3. Ingress (L7 Routing & Multiplexing)

|       | `LoadBalancer` Service              | Ingress                            |
| ----- | ----------------------------------- | ---------------------------------- |
| Layer | L4: Transport Layer                 | L7: Application Layer              |
| 식별 기준 | IP 주소 + Port num <br>(TCP/UDP 프로토콜) | HTTP Header<br>(Host 필드, URI path) |
| 매핑    | 1:1<br>Public IP 1개 : Service 1개    | 1:N<br>Public IP 1개 : Service N개   |
| 동작 방식 | NAT / Tunneling                     | Reverse Proxy                      |
- Ingress Controller가 Reverse Proxy로 동작
	- 클라이언트와의 TCP 연결을 수립한 후,
	- HTTP 패킷을 복호화 및 분석하여 
	- Ingress Rule(라우팅 규칙)에 따라 적절한 백엔드 서비스로 요청 생성 및 전달
##### 3-1. Deployment Controller 리소스 생성
```
$ kubectl create deploy ing-deploy --image nginx --port 80 --replicas 3 -n jde-ns
```
- `nginx` 컨테이너를 실행하는 Pod 3개를 Worker Node에 스케줄링
- 트래픽을 처리할 실제 애플케이션 Instance (Workload) 확보
<img width="2441" height="193" alt="Pasted image 20260107120613" src="https://github.com/user-attachments/assets/9ed77786-7a81-421c-978f-38eb065a7076" />

##### 3-2. `ClusterIP` 타입 Service 생성
- 앞서 만든 Deployment Pod들을 논리적 그룹으로 묶고,
- `ClusterIP`를 할당받아 `ing-svc`를 생성한다.
	- 클러스터 내부 Overlay Network에서만 접근 가능한 고정 IP
	- `--type=ClusterIP`: Cluster 내부의 Ingress Controller만이 서비스에 접근하여 트래픽을 전달하도록 제한
<img width="2387" height="375" alt="Pasted image 20260107120636" src="https://github.com/user-attachments/assets/d520b7cb-a080-491c-944a-03d4862a5926" />

##### 3-3. Ingress 리소스 생성 (Routing Configuration)
```sh
$ kubectl create ing ingress --class=nginx \
 --rule=ing.133.186.xxx.xxx.nip.io/=ing-svc:80 -n jde-ns
```
- `--rule` : Host-based Routing
	- 입력한 host로 들어오는 트래픽 식별
- `/=ing-svc:80`: Upstream Binding
	- 해당 Host의 Root 요청을
	- 내부 서비스인 `ing-svc`의 `80`포트로 Forwarding하도록 명시
- `--class=nginx`: Ingress Class
	- 클러스터 내의 Nginx Ingress Controller가 주체
```sh
# Ingress Configuration Sync Status 확인
$ kubectl describe ing ingress -n jde-ns
```
- `Host`와 `Backends`의 매핑 정보 확인
	- `Host`: `ing.133.186.xxx.xxx.nip.io`
	- `Path`: `/<none>`
	- `Backends`: `ing-svc:80`
<img width="2395" height="606" alt="Pasted image 20260107120759" src="https://github.com/user-attachments/assets/b57fd5ae-f126-4d73-b6d4-ff0652a981bb" />

- Ingress Controller가 `ing-svc`를 통해 실제 Pod의 IP(Endpoints)를 성공적으로 탐색
- `Events`: `Normal Sync ... Scheduled for sync`
	- Nginx Ingress Controller가 API Server로부터
	- 새로운 Ingress 리소스 생성을 감지하고
	- `nginx.conf` 설정을 Reload하여 라우팅 규칙을 적용했음을 의미
##### 3-4. 브라우저 접속
1. DNS Resolution
	- 클라이언트가 `nip.io` DNS 서버를 통해 도메인을 Floating IP로 해석
2. External Access
	- 트래픽이 NHN Gateway $\rightarrow$ External IP $\rightarrow$ Ingress Controller Pod로 유입
3. L7 Inspection
	- Nginx 프로세스가 HTTP 패킷의 Host Header 검사
4. Proxy Pass
	- 설정된 규칙에 따라 트래픽을 `ing-svc`의 Endpoints 중 하나로 라우팅
5. Response
<img width="852" height="440" alt="Pasted image 20260107120814" src="https://github.com/user-attachments/assets/26cf15e8-2f37-4a0b-828a-9704f2d5e9da" />

#### 4. Ingress Advanced Routing (Path-based vs. Host-based)

|        | Path-based Routing     | Host-based Routing   |
| ------ | ---------------------- | -------------------- |
| 라우팅 기준 | Request URI (URL Path) | HTTP Host Header     |
| 식별 데이터 | `GET /hello HTTP/1.1`  | `Host: hello.nip.io` |
| URL 구조 | `...nip.io/hello`      | `hello. .nip.io`     |
| DNS 설정 | 단일 레코드                 | 와일드카드 또는 다중 레코드      |
##### 4-1. Backend Workload 배포: Deployment
```sh
# 실습2,3,4) Ingress - 240, 263, 286p
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app: "grafana"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "grafana"
  template:
    metadata:
      labels:
        app: "grafana"
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - name: http
          containerPort: 3000
---
# 실습2,3,4) Ingress - 243, 266, 289p
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      name: hello
      labels:
        app: hello
    spec:
      containers:
      - name: nginx
        image: nginxdemos/hello:plain-text
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
---
# 실습2,3,4) Ingress - 246, 269, 292p
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd
  labels:
    app: "httpd"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "httpd"
  template:
    metadata:
      labels:
        app: "httpd"
    spec:
      containers:
      - name: httpd
        image: httpd:latest
        ports:
        - name: http
          containerPort: 80

```
##### 4-2. Backend Workload 배포: Service(`ClusterIP`) 
```sh
# 실습2,3,4) Ingress - 249, 272, 295p
apiVersion: v1
kind: Service
metadata:
  name: grafana
  labels:
    app: "grafana"
spec:
  type: ClusterIP
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 3000
  selector:
    app: "grafana"
---
# 실습2,3,4) Ingress - 252, 275, 298p
apiVersion: v1
kind: Service
metadata:
  name: hello
  labels:
    app: hello
spec:
  type: ClusterIP
  ports:
  - name: http
    protocol: TCP
    port: 8080
    targetPort: 80
  selector:
    app: hello
---
# 실습2,3,4) Ingress - 255, 278, 301p
apiVersion: v1
kind: Service
metadata:
  name: httpd
  labels:
    app: "httpd"
spec:
  type: ClusterIP
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  selector:
    app: "httpd"
```
##### 4-3. Path-based Routing
<img width="2470" height="730" alt="Pasted image 20260107123043" src="https://github.com/user-attachments/assets/66869a61-93ce-407d-af48-fc0ca626f111" />

```sh
# deployment 배포 (파일 설정 잘못함 실수임)
$ kubectl apply -f deploy-ing.yaml
# service 배포
$ kubectl apply -f svc-ing.yaml
# 결과
service/grafana created
service/hello created
service/httpd created
```
<img width="1976" height="788" alt="Pasted image 20260107123532" src="https://github.com/user-attachments/assets/d3bc3738-57ad-4e49-98d4-ab1d942c81af" />

- `kubectl` 배포 시 파일 설정 잘못해서 이렇게 됨
<img width="2430" height="1255" alt="image" src="https://github.com/user-attachments/assets/cd6fe2c9-ee24-47c3-b646-7b22a5829b28" />

- Paths:
	- `/` $\rightarrow$ `grafana-service`
	- `/hello` $\rightarrow$ `hello-service`
	- `/httpd` $\rightarrow$ `httpd-service`
<img width="2007" height="695" alt="Pasted image 20260107123102" src="https://github.com/user-attachments/assets/7097ff51-915e-48b5-9e35-dc1e24050d37" />

- Ingress Controller(Nginx)가 HTTP Request URI 파싱
- L7 Context Switching
	- 단일 도메인으로 들어온 트래픽을 
	- URI Prefix에 따라 
	- 서로 다른 Upstream Service로 Multiplexing
###### 브라우저 접속
- `jde.133.186.xxx.xxx.nip.io/grafana`
<img width="1863" height="837" alt="image" src="https://github.com/user-attachments/assets/6c5eb3c2-1f90-448b-b368-db7ecbe2419d" />
<img width="1168" height="222" alt="image" src="https://github.com/user-attachments/assets/e304d04a-04a4-4a1e-8464-9e0d269d9f2e" />

##### 4-4. Host-based Routing
```sh
# ing2.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              name: http
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              name: http
  - http:
      paths:
      - path: /httpd
        pathType: Prefix
        backend:
          service:
            name: httpd
            port:
              name: http
```
- `pathType: Prefix`
- 클라이언트가 요청에 담아 보낸 `Host` 필드 값을 기준으로 라우팅 대상 결정
<img width="2337" height="989" alt="image" src="https://github.com/user-attachments/assets/427e18e0-e7d7-4126-9e2a-fc85e4c80439" />
<img width="1992" height="694" alt="image" src="https://github.com/user-attachments/assets/6d2f717b-0583-4e3d-9471-272356f7a84c" />

###### 브라우저 접속
- `grafana.133.186.xxx.xxx.nip.io`
- `hello.133.186.xxx.xxx.nip.io`
- `httpd.133.186.xxx.xxx.nip.io`

<img width="1853" height="774" alt="image" src="https://github.com/user-attachments/assets/8b6a9341-8746-4508-90b9-2423397dc1a9" />
<img width="1150" height="211" alt="image" src="https://github.com/user-attachments/assets/cd142123-049e-4c2c-b3af-a2ebb4c0ba20" />
<img width="1020" height="185" alt="image" src="https://github.com/user-attachments/assets/33c3fe18-e8fa-4e2a-832d-4987cdd56f9a" />
